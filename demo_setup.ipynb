{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from dime.engine import SearchEngine\n",
    "import torch\n",
    "import torchvision\n",
    "from train_model import IntermodalTripletNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "Loading in word vectors...\n",
      "Done\n",
      "\n",
      "Loading NUS_WIDE dataset...\n",
      "Done\n",
      "\n",
      "Making training and validation indices...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/dime/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/161789 (0%)]\tLoss: 9.971259\n",
      "Train: [6400/161789 (5%)]\tLoss: 9.510913\n",
      "Train: [12800/161789 (10%)]\tLoss: 8.882437\n",
      "Train: [19200/161789 (15%)]\tLoss: 8.740899\n",
      "Train: [25600/161789 (20%)]\tLoss: 7.867095\n",
      "Train: [32000/161789 (25%)]\tLoss: 7.880015\n",
      "Train: [38400/161789 (30%)]\tLoss: 7.594324\n",
      "Train: [44800/161789 (35%)]\tLoss: 7.399308\n",
      "Train: [51200/161789 (40%)]\tLoss: 7.340761\n",
      "Train: [57600/161789 (44%)]\tLoss: 7.441620\n",
      "Train: [64000/161789 (49%)]\tLoss: 7.205539\n",
      "Train: [70400/161789 (54%)]\tLoss: 7.152166\n",
      "Train: [76800/161789 (59%)]\tLoss: 6.838610\n",
      "Train: [83200/161789 (64%)]\tLoss: 6.801349\n",
      "Train: [89600/161789 (69%)]\tLoss: 6.745960\n",
      "Train: [96000/161789 (74%)]\tLoss: 6.599819\n",
      "Train: [102400/161789 (79%)]\tLoss: 6.365149\n",
      "Train: [108800/161789 (84%)]\tLoss: 6.552379\n",
      "Train: [115200/161789 (89%)]\tLoss: 6.405799\n",
      "Train: [121600/161789 (94%)]\tLoss: 6.362945\n",
      "Train: [128000/161789 (99%)]\tLoss: 6.305059\n",
      "Epoch: 1/15. Train set: Average loss: 7.2886\n",
      "Epoch: 1/15. Validation set: Average loss: 6.4048\n",
      "Train: [0/161789 (0%)]\tLoss: 6.620896\n",
      "Train: [6400/161789 (5%)]\tLoss: 6.349639\n",
      "Train: [12800/161789 (10%)]\tLoss: 6.295342\n",
      "Train: [19200/161789 (15%)]\tLoss: 6.226756\n",
      "Train: [25600/161789 (20%)]\tLoss: 6.225272\n",
      "Train: [32000/161789 (25%)]\tLoss: 6.203604\n",
      "Train: [38400/161789 (30%)]\tLoss: 6.185748\n",
      "Train: [44800/161789 (35%)]\tLoss: 6.179419\n",
      "Train: [51200/161789 (40%)]\tLoss: 6.071612\n",
      "Train: [57600/161789 (44%)]\tLoss: 6.093276\n",
      "Train: [64000/161789 (49%)]\tLoss: 6.119849\n",
      "Train: [70400/161789 (54%)]\tLoss: 6.026016\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.990516\n",
      "Train: [83200/161789 (64%)]\tLoss: 6.120908\n",
      "Train: [89600/161789 (69%)]\tLoss: 6.020650\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.829639\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.893542\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.903244\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.791422\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.843109\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.959730\n",
      "Epoch: 2/15. Train set: Average loss: 6.0639\n",
      "Epoch: 2/15. Validation set: Average loss: 5.8496\n",
      "Train: [0/161789 (0%)]\tLoss: 5.872526\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.871515\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.759574\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.646145\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.785322\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.888493\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.883929\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.685152\n",
      "Train: [51200/161789 (40%)]\tLoss: 5.726371\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.752231\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.831815\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.760591\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.764394\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.772872\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.826684\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.766565\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.674490\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.642592\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.818160\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.743696\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.537887\n",
      "Epoch: 3/15. Train set: Average loss: 5.7553\n",
      "Epoch: 3/15. Validation set: Average loss: 5.7267\n",
      "Train: [0/161789 (0%)]\tLoss: 5.543091\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.718253\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.677899\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.609910\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.660151\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.533767\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.608833\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.676459\n",
      "Train: [51200/161789 (40%)]\tLoss: 5.556902\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.561871\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.466861\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.584592\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.640755\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.637961\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.754129\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.515468\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.653612\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.572437\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.546719\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.587460\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.569338\n",
      "Epoch: 4/15. Train set: Average loss: 5.6030\n",
      "Epoch: 4/15. Validation set: Average loss: 5.5480\n",
      "Train: [0/161789 (0%)]\tLoss: 5.537798\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.491793\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.484549\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.599145\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.637759\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.655393\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.407742\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.433727\n",
      "Train: [51200/161789 (40%)]\tLoss: 5.481458\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.343390\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.434203\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.385860\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.452959\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.361303\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.445274\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.412012\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.398885\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.378121\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.378845\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.426861\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.447127\n",
      "Epoch: 5/15. Train set: Average loss: 5.4535\n",
      "Epoch: 5/15. Validation set: Average loss: 5.4339\n",
      "Train: [0/161789 (0%)]\tLoss: 5.037192\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.411225\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.403621\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.240648\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.413092\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.208393\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.352251\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.447667\n",
      "Train: [51200/161789 (40%)]\tLoss: 5.443090\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.390435\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.295554\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.373474\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.194711\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.437385\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.275151\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.331730\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.353298\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.431244\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.281759\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.264312\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.335325\n",
      "Epoch: 6/15. Train set: Average loss: 5.3450\n",
      "Epoch: 6/15. Validation set: Average loss: 5.3090\n",
      "Train: [0/161789 (0%)]\tLoss: 5.142446\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.280024\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.332728\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.366563\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.175252\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.291473\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.189099\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.264401\n",
      "Train: [51200/161789 (40%)]\tLoss: 5.303945\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.352319\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.274710\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.251418\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.186041\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.254716\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.332007\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.227539\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.439888\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.088587\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.262479\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.309920\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.314931\n",
      "Epoch: 7/15. Train set: Average loss: 5.2729\n",
      "Epoch: 7/15. Validation set: Average loss: 5.2726\n",
      "Train: [0/161789 (0%)]\tLoss: 5.323968\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.159864\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.996272\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.973992\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.198613\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.015300\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.085940\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.099287\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.983884\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.023717\n",
      "Train: [64000/161789 (49%)]\tLoss: 5.097286\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.062299\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.078309\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.017411\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.044980\n",
      "Train: [96000/161789 (74%)]\tLoss: 5.106329\n",
      "Train: [102400/161789 (79%)]\tLoss: 5.028946\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.115763\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.970281\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.887810\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.070517\n",
      "Epoch: 8/15. Train set: Average loss: 5.0518\n",
      "Epoch: 8/15. Validation set: Average loss: 5.0860\n",
      "Train: [0/161789 (0%)]\tLoss: 4.442952\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.991412\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.868356\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.023676\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.050229\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.983278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [38400/161789 (30%)]\tLoss: 4.940617\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.941726\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.977556\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.822852\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.884081\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.932818\n",
      "Train: [76800/161789 (59%)]\tLoss: 5.003038\n",
      "Train: [83200/161789 (64%)]\tLoss: 4.793852\n",
      "Train: [89600/161789 (69%)]\tLoss: 5.051009\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.992085\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.943753\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.995441\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.938844\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.082080\n",
      "Train: [128000/161789 (99%)]\tLoss: 4.940906\n",
      "Epoch: 9/15. Train set: Average loss: 4.9578\n",
      "Epoch: 9/15. Validation set: Average loss: 5.0835\n",
      "Train: [0/161789 (0%)]\tLoss: 6.115368\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.877423\n",
      "Train: [12800/161789 (10%)]\tLoss: 5.126612\n",
      "Train: [19200/161789 (15%)]\tLoss: 5.160595\n",
      "Train: [25600/161789 (20%)]\tLoss: 5.005803\n",
      "Train: [32000/161789 (25%)]\tLoss: 5.031905\n",
      "Train: [38400/161789 (30%)]\tLoss: 4.990134\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.964639\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.922781\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.856635\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.942992\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.978817\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.975788\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.007567\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.997262\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.791466\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.959711\n",
      "Train: [108800/161789 (84%)]\tLoss: 5.031971\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.911121\n",
      "Train: [121600/161789 (94%)]\tLoss: 5.062723\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.011445\n",
      "Epoch: 10/15. Train set: Average loss: 4.9811\n",
      "Epoch: 10/15. Validation set: Average loss: 5.0143\n",
      "Train: [0/161789 (0%)]\tLoss: 5.474857\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.960383\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.838093\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.950248\n",
      "Train: [25600/161789 (20%)]\tLoss: 4.934193\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.884534\n",
      "Train: [38400/161789 (30%)]\tLoss: 4.889039\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.889780\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.983146\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.926569\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.921553\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.811284\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.832261\n",
      "Train: [83200/161789 (64%)]\tLoss: 4.892755\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.954958\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.957727\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.807370\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.939002\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.045906\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.926270\n",
      "Train: [128000/161789 (99%)]\tLoss: 4.790776\n",
      "Epoch: 11/15. Train set: Average loss: 4.9074\n",
      "Epoch: 11/15. Validation set: Average loss: 5.0279\n",
      "Train: [0/161789 (0%)]\tLoss: 5.271097\n",
      "Train: [6400/161789 (5%)]\tLoss: 5.042086\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.964854\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.868751\n",
      "Train: [25600/161789 (20%)]\tLoss: 4.911829\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.854026\n",
      "Train: [38400/161789 (30%)]\tLoss: 5.086575\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.880315\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.937027\n",
      "Train: [57600/161789 (44%)]\tLoss: 5.028613\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.861410\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.890737\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.757147\n",
      "Train: [83200/161789 (64%)]\tLoss: 5.063609\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.931546\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.924966\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.923105\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.880201\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.927602\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.864523\n",
      "Train: [128000/161789 (99%)]\tLoss: 4.888535\n",
      "Epoch: 12/15. Train set: Average loss: 4.9236\n",
      "Epoch: 12/15. Validation set: Average loss: 5.0055\n",
      "Train: [0/161789 (0%)]\tLoss: 5.127666\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.882340\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.918964\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.857045\n",
      "Train: [25600/161789 (20%)]\tLoss: 4.957058\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.878458\n",
      "Train: [38400/161789 (30%)]\tLoss: 4.971202\n",
      "Train: [44800/161789 (35%)]\tLoss: 5.058164\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.843184\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.861048\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.852462\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.846390\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.885751\n",
      "Train: [83200/161789 (64%)]\tLoss: 4.992940\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.878057\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.918645\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.901322\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.869316\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.887025\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.894399\n",
      "Train: [128000/161789 (99%)]\tLoss: 4.821742\n",
      "Epoch: 13/15. Train set: Average loss: 4.8955\n",
      "Epoch: 13/15. Validation set: Average loss: 5.0009\n",
      "Train: [0/161789 (0%)]\tLoss: 4.912930\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.907589\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.975693\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.811556\n",
      "Train: [25600/161789 (20%)]\tLoss: 4.995530\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.915296\n",
      "Train: [38400/161789 (30%)]\tLoss: 4.834992\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.864682\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.921304\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.839501\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.987933\n",
      "Train: [70400/161789 (54%)]\tLoss: 5.072186\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.946078\n",
      "Train: [83200/161789 (64%)]\tLoss: 4.750522\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.819656\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.846912\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.895010\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.788805\n",
      "Train: [115200/161789 (89%)]\tLoss: 5.005294\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.869827\n",
      "Train: [128000/161789 (99%)]\tLoss: 5.034859\n",
      "Epoch: 14/15. Train set: Average loss: 4.9024\n",
      "Epoch: 14/15. Validation set: Average loss: 4.9739\n",
      "Train: [0/161789 (0%)]\tLoss: 4.773633\n",
      "Train: [6400/161789 (5%)]\tLoss: 4.952487\n",
      "Train: [12800/161789 (10%)]\tLoss: 4.900951\n",
      "Train: [19200/161789 (15%)]\tLoss: 4.894989\n",
      "Train: [25600/161789 (20%)]\tLoss: 4.791549\n",
      "Train: [32000/161789 (25%)]\tLoss: 4.983212\n",
      "Train: [38400/161789 (30%)]\tLoss: 4.855307\n",
      "Train: [44800/161789 (35%)]\tLoss: 4.738726\n",
      "Train: [51200/161789 (40%)]\tLoss: 4.867074\n",
      "Train: [57600/161789 (44%)]\tLoss: 4.861741\n",
      "Train: [64000/161789 (49%)]\tLoss: 4.971264\n",
      "Train: [70400/161789 (54%)]\tLoss: 4.887126\n",
      "Train: [76800/161789 (59%)]\tLoss: 4.909385\n",
      "Train: [83200/161789 (64%)]\tLoss: 4.782677\n",
      "Train: [89600/161789 (69%)]\tLoss: 4.850691\n",
      "Train: [96000/161789 (74%)]\tLoss: 4.876487\n",
      "Train: [102400/161789 (79%)]\tLoss: 4.948675\n",
      "Train: [108800/161789 (84%)]\tLoss: 4.834705\n",
      "Train: [115200/161789 (89%)]\tLoss: 4.863510\n",
      "Train: [121600/161789 (94%)]\tLoss: 4.967423\n",
      "Train: [128000/161789 (99%)]\tLoss: 4.882201\n",
      "Epoch: 15/15. Train set: Average loss: 4.8800\n",
      "Epoch: 15/15. Validation set: Average loss: 4.9918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_model import main\n",
    "import gc\n",
    "main(15, \"resnet18\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_params = {\n",
    "    \"name\": \"demo_engine\",\n",
    "    \"cuda\": True,\n",
    "    \"verbose\": True,\n",
    "    \"dataset_dir\": \"data/\",\n",
    "    \"index_dir\": \"indexes/\",\n",
    "    \"model_dir\": \"models/\",\n",
    "    \"embedding_dir\": \"embeddings/\",\n",
    "    \"modalities\": [\"text\", \"image\", \"audio\", \"video\"]   \n",
    "}\n",
    "\n",
    "engine = SearchEngine(engine_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'resnet152' added\n"
     ]
    }
   ],
   "source": [
    "r152_features_params = {\n",
    "    \"name\": \"resnet152\",\n",
    "    \"output_dim\": (2048,),\n",
    "    \"modalities\": [\"image\"],\n",
    "    \"embedding_nets\": [torch.nn.Sequential(*list(torchvision.models.resnet152(pretrained=True).children())[:-1])],\n",
    "    \"input_dim\": [(3, 224, 224)],\n",
    "    \"desc\": \"Resnet152 with the last layer removed for feature extraction\"\n",
    "} \n",
    "\n",
    "engine.add_model(r152_features_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'resnet18' added\n"
     ]
    }
   ],
   "source": [
    "r18_features_params = {\n",
    "    \"name\": \"resnet18\",\n",
    "    \"output_dim\": (512,),\n",
    "    \"modalities\": [\"image\"],\n",
    "    \"embedding_nets\": [torch.nn.Sequential(*list(torchvision.models.resnet18(pretrained=True).children())[:-1])],\n",
    "    \"input_dim\": [(3, 224, 224)],\n",
    "    \"desc\": \"Resnet18 with the last layer removed for feature extraction\"\n",
    "} \n",
    "\n",
    "engine.add_model(r18_features_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'cm-r152-5epochs' added\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/models/resnet152_5epochs.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "demo_model1_params = {\n",
    "    \"name\": \"cm-r152-5epochs\",\n",
    "    \"output_dim\": (200,),\n",
    "    \"modalities\": [\"image\", \"text\"],\n",
    "    \"embedding_nets\": [model.modalityOneNet, model.modalityTwoNet],\n",
    "    \"input_dim\": [(2048,), (300,)],\n",
    "    \"desc\": \"5 epoch adversarial cross-modal triplet-loss retrieval trained with resnet152 features and wiki word2vec\"\n",
    "}\n",
    "\n",
    "engine.add_model(demo_model1_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'cm-r152-15epochs' added\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/models/resnet152_15epochs.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "demo_model1_params = {\n",
    "    \"name\": \"cm-r152-15epochs\",\n",
    "    \"output_dim\": (200,),\n",
    "    \"modalities\": [\"image\", \"text\"],\n",
    "    \"embedding_nets\": [model.modalityOneNet, model.modalityTwoNet],\n",
    "    \"input_dim\": [(2048,), (300,)],\n",
    "    \"desc\": \"15 epoch adversarial cross-modal triplet-loss retrieval trained with resnet152 features and wiki word2vec\"\n",
    "}\n",
    "\n",
    "engine.add_model(demo_model1_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'cm-r18-15epochs' added\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/models/resnet18_15epochs.p\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "demo_model1_params = {\n",
    "    \"name\": \"cm-r18-15epochs\",\n",
    "    \"output_dim\": (200,),\n",
    "    \"modalities\": [\"image\", \"text\"],\n",
    "    \"embedding_nets\": [model.modalityOneNet, model.modalityTwoNet],\n",
    "    \"input_dim\": [(512,), (300,)],\n",
    "    \"desc\": \"15 epoch adversarial cross-modal triplet-loss retrieval trained with resnet18 features and wiki word2vec\"\n",
    "}\n",
    "\n",
    "engine.add_model(demo_model1_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.add_preprocessor(\"cm-r152-5epochs\", \"image\", \"resnet152\")\n",
    "engine.add_preprocessor(\"cm-r152-15epochs\", \"image\", \"resnet152\")\n",
    "engine.add_preprocessor(\"cm-r18-15epochs\", \"image\", \"resnet18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'nuswide' added\n"
     ]
    }
   ],
   "source": [
    "nuswide_params = {\n",
    "    \"name\": \"nuswide\",\n",
    "    \"data_dir\": \"Flickr/\",\n",
    "    \"transform\": torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224,224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]),\n",
    "    \"dim\": (3, 224, 224),\n",
    "    \"modality\": \"image\",\n",
    "    \"desc\": \"The nuswide dataset\"\n",
    "}\n",
    "\n",
    "engine.add_dataset(nuswide_params, force_add = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'wiki_word2vec' added\n"
     ]
    }
   ],
   "source": [
    "with open(\"pickles/word_embeddings/word_embeddings_tensors.p\", \"rb\") as f:\n",
    "    wiki = pickle.load(f)\n",
    "    \n",
    "wiki_word2vec_params = {\n",
    "    \"name\": \"wiki_word2vec\",\n",
    "    \"data\": wiki,\n",
    "    \"modality\": \"text\",\n",
    "    \"dim\": (300,),\n",
    "    \"desc\": \"one million word2vec entries trained on English Wikipedia\"\n",
    "}\n",
    "\n",
    "engine.add_dataset(wiki_word2vec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1= {\n",
    "    \"name\": \"NUSWIDE (cm-r152-5epochs)\",\n",
    "    \"model_name\": \"cm-r152-5epochs\",\n",
    "    \"dataset_name\": \"nuswide\",\n",
    "    \"desc\": \"The index corresponding to cm-r152-5epochs model and nuswide\"\n",
    "}\n",
    "\n",
    "ind2 = {\n",
    "    \"name\": \"WIKI WORD2VEC (cm-r152-5epochs)\",\n",
    "    \"model_name\": \"cm-r152-5epochs\",\n",
    "    \"dataset_name\": \"wiki_word2vec\",\n",
    "    \"desc\": \"The index corresponding to cm-r152-5epochs model and wiki_word2vec\"\n",
    "}\n",
    "\n",
    "ind3 = {\n",
    "    \"name\": \"NUSWIDE (cm-r152-15epochs)\",\n",
    "    \"model_name\": \"cm-r152-15epochs\",\n",
    "    \"dataset_name\": \"nuswide\",\n",
    "    \"desc\": \"The index corresponding to cm-r152-15epochs model and nuswide\"\n",
    "}\n",
    "ind4 = {\n",
    "    \"name\": \"WIKI WORD2VEC (cm-r152-15epochs)\",\n",
    "    \"model_name\": \"cm-r152-15epochs\",\n",
    "    \"dataset_name\": \"wiki_word2vec\",\n",
    "    \"desc\": \"The index corresponding to cm-r152-15epochs model and wiki_word2vec\"\n",
    "}\n",
    "ind5 = {\n",
    "    \"name\": \"NUSWIDE (cm-r18-15epochs)\",\n",
    "    \"model_name\": \"cm-r18-15epochs\",\n",
    "    \"dataset_name\": \"nuswide\",\n",
    "    \"desc\": \"The index corresponding to cm-r18-15epochs model and nuswide\"\n",
    "}\n",
    "ind6 = {\n",
    "    \"name\": \"WIKI WORD2VEC (cm-r18-15epochs)\",\n",
    "    \"model_name\": \"cm-r18-15epochs\",\n",
    "    \"dataset_name\": \"wiki_word2vec\",\n",
    "    \"desc\": \"The index corresponding to cm-r18-15epochs model and wiki_word2vec\"\n",
    "}\n",
    "\n",
    "ind7 = {\n",
    "    \"name\": \"NUSWIDE (resnet152)\",\n",
    "    \"model_name\": \"resnet152\",\n",
    "    \"dataset_name\": \"nuswide\",\n",
    "    \"desc\": \"The index corresponding to resnet152 and nuswide\"\n",
    "}\n",
    "\n",
    "ind8 = {\n",
    "    \"name\": \"NUSWIDE (resnet18)\",\n",
    "    \"model_name\": \"resnet18\",\n",
    "    \"dataset_name\": \"nuswide\",\n",
    "    \"desc\": \"The index corresponding to resnet18 and nuswide\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building cm-r152-5epochs, nuswide index\n",
      "Loading batch 0 of 8427\n",
      "Processing batch 1000 of 8427\n",
      "Processing batch 2000 of 8427\n",
      "Processing batch 3000 of 8427\n",
      "Processing batch 4000 of 8427\n",
      "Processing batch 5000 of 8427\n",
      "Processing batch 6000 of 8427\n",
      "Processing batch 7000 of 8427\n",
      "Processing batch 8000 of 8427\n",
      "Finished building index NUSWIDE (cm-r152-5epochs) in 1528.1353 seconds.\n",
      "Building cm-r152-5epochs, wiki_word2vec index\n",
      "Processing batch 0 of 31250\n",
      "Processing batch 1000 of 31250\n",
      "Processing batch 2000 of 31250\n",
      "Processing batch 3000 of 31250\n",
      "Processing batch 4000 of 31250\n",
      "Processing batch 5000 of 31250\n",
      "Processing batch 6000 of 31250\n",
      "Processing batch 7000 of 31250\n",
      "Processing batch 8000 of 31250\n",
      "Processing batch 9000 of 31250\n",
      "Processing batch 10000 of 31250\n",
      "Processing batch 11000 of 31250\n",
      "Processing batch 12000 of 31250\n",
      "Processing batch 13000 of 31250\n",
      "Processing batch 14000 of 31250\n",
      "Processing batch 15000 of 31250\n",
      "Processing batch 16000 of 31250\n",
      "Processing batch 17000 of 31250\n",
      "Processing batch 18000 of 31250\n",
      "Processing batch 19000 of 31250\n",
      "Processing batch 20000 of 31250\n",
      "Processing batch 21000 of 31250\n",
      "Processing batch 22000 of 31250\n",
      "Processing batch 23000 of 31250\n",
      "Processing batch 24000 of 31250\n",
      "Processing batch 25000 of 31250\n",
      "Processing batch 26000 of 31250\n",
      "Processing batch 27000 of 31250\n",
      "Processing batch 28000 of 31250\n",
      "Processing batch 29000 of 31250\n",
      "Processing batch 30000 of 31250\n",
      "Processing batch 31000 of 31250\n",
      "Finished building index WIKI WORD2VEC (cm-r152-5epochs) in 20.2355 seconds.\n",
      "Building cm-r152-15epochs, nuswide index\n",
      "Processing batch 0 of 8427\n",
      "Processing batch 1000 of 8427\n",
      "Processing batch 2000 of 8427\n",
      "Processing batch 3000 of 8427\n",
      "Processing batch 4000 of 8427\n",
      "Processing batch 5000 of 8427\n",
      "Processing batch 6000 of 8427\n",
      "Processing batch 7000 of 8427\n",
      "Processing batch 8000 of 8427\n",
      "Finished building index NUSWIDE (cm-r152-15epochs) in 1533.9701 seconds.\n",
      "Building cm-r152-15epochs, wiki_word2vec index\n",
      "Processing batch 0 of 31250\n",
      "Processing batch 1000 of 31250\n",
      "Processing batch 2000 of 31250\n",
      "Processing batch 3000 of 31250\n",
      "Processing batch 4000 of 31250\n",
      "Processing batch 5000 of 31250\n",
      "Processing batch 6000 of 31250\n",
      "Processing batch 7000 of 31250\n",
      "Processing batch 8000 of 31250\n",
      "Processing batch 9000 of 31250\n",
      "Processing batch 10000 of 31250\n",
      "Processing batch 11000 of 31250\n",
      "Processing batch 12000 of 31250\n",
      "Processing batch 13000 of 31250\n",
      "Processing batch 14000 of 31250\n",
      "Processing batch 15000 of 31250\n",
      "Processing batch 16000 of 31250\n",
      "Processing batch 17000 of 31250\n",
      "Processing batch 18000 of 31250\n",
      "Processing batch 19000 of 31250\n",
      "Processing batch 20000 of 31250\n",
      "Processing batch 21000 of 31250\n",
      "Processing batch 22000 of 31250\n",
      "Processing batch 23000 of 31250\n",
      "Processing batch 24000 of 31250\n",
      "Processing batch 25000 of 31250\n",
      "Processing batch 26000 of 31250\n",
      "Processing batch 27000 of 31250\n",
      "Processing batch 28000 of 31250\n",
      "Processing batch 29000 of 31250\n",
      "Processing batch 30000 of 31250\n",
      "Processing batch 31000 of 31250\n",
      "Finished building index WIKI WORD2VEC (cm-r152-15epochs) in 20.2205 seconds.\n",
      "Building cm-r18-15epochs, nuswide index\n",
      "Processing batch 0 of 8427\n",
      "Processing batch 1000 of 8427\n",
      "Processing batch 2000 of 8427\n",
      "Processing batch 3000 of 8427\n",
      "Processing batch 4000 of 8427\n",
      "Processing batch 5000 of 8427\n",
      "Processing batch 6000 of 8427\n",
      "Processing batch 7000 of 8427\n",
      "Processing batch 8000 of 8427\n",
      "Finished building index NUSWIDE (cm-r18-15epochs) in 692.2221 seconds.\n",
      "Building cm-r18-15epochs, wiki_word2vec index\n",
      "Processing batch 0 of 31250\n",
      "Processing batch 1000 of 31250\n",
      "Processing batch 2000 of 31250\n",
      "Processing batch 3000 of 31250\n",
      "Processing batch 4000 of 31250\n",
      "Processing batch 5000 of 31250\n",
      "Processing batch 6000 of 31250\n",
      "Processing batch 7000 of 31250\n",
      "Processing batch 8000 of 31250\n",
      "Processing batch 9000 of 31250\n",
      "Processing batch 10000 of 31250\n",
      "Processing batch 11000 of 31250\n",
      "Processing batch 12000 of 31250\n",
      "Processing batch 13000 of 31250\n",
      "Processing batch 14000 of 31250\n",
      "Processing batch 15000 of 31250\n",
      "Processing batch 16000 of 31250\n",
      "Processing batch 17000 of 31250\n",
      "Processing batch 18000 of 31250\n",
      "Processing batch 19000 of 31250\n",
      "Processing batch 20000 of 31250\n",
      "Processing batch 21000 of 31250\n",
      "Processing batch 22000 of 31250\n",
      "Processing batch 23000 of 31250\n",
      "Processing batch 24000 of 31250\n",
      "Processing batch 25000 of 31250\n",
      "Processing batch 26000 of 31250\n",
      "Processing batch 27000 of 31250\n",
      "Processing batch 28000 of 31250\n",
      "Processing batch 29000 of 31250\n",
      "Processing batch 30000 of 31250\n",
      "Processing batch 31000 of 31250\n",
      "Finished building index WIKI WORD2VEC (cm-r18-15epochs) in 19.9383 seconds.\n",
      "Building resnet152, nuswide index\n",
      "Processing batch 0 of 8427\n",
      "Processing batch 1000 of 8427\n",
      "Processing batch 2000 of 8427\n",
      "Processing batch 3000 of 8427\n",
      "Processing batch 4000 of 8427\n",
      "Processing batch 5000 of 8427\n",
      "Processing batch 6000 of 8427\n",
      "Processing batch 7000 of 8427\n",
      "Processing batch 8000 of 8427\n",
      "Finished building index NUSWIDE (resnet152) in 1542.8857 seconds.\n",
      "Building resnet18, nuswide index\n",
      "Processing batch 0 of 8427\n",
      "Processing batch 1000 of 8427\n",
      "Processing batch 2000 of 8427\n",
      "Processing batch 3000 of 8427\n",
      "Processing batch 4000 of 8427\n",
      "Processing batch 5000 of 8427\n",
      "Processing batch 6000 of 8427\n",
      "Processing batch 7000 of 8427\n",
      "Processing batch 8000 of 8427\n",
      "Finished building index NUSWIDE (resnet18) in 692.6513 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NUSWIDE (resnet18)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.build_index(ind1, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind2, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind3, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind4, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind5, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind6, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind7, batch_size = BATCH_SIZE)\n",
    "engine.build_index(ind8, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save(save_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dime",
   "language": "python",
   "name": "dime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
